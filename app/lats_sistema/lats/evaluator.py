# lats/evaluator.py
import json
from typing import Dict, Any, List
from langchain_core.prompts import ChatPromptTemplate
from app.lats_sistema.models.llm import llm_json
from app.lats_sistema.lats.utils import formatar_filhos
from app.lats_sistema.utils.json_utils import invoke_json

# ---------------------------------------------------------
# Avalia√ß√£o via LLM ‚Äì compara EVENTO vs FILHOS do n√≥ atual
# ---------------------------------------------------------
def avaliar_filhos_llm(node: Dict[str, Any], descricao_evento: str, contexto_normativo: str):
    filhos = node.get("subnodos", [])
    if not filhos:
        return []

    filhos_formatados = formatar_filhos(node)
    contexto_normativo = contexto_normativo or ""

    prompt = ChatPromptTemplate.from_template("""
Voc√™ √© um CLASSIFICADOR NORMATIVO PETROBRAS/ANP baseado em uma √ÅRVORE DE DECIS√ÉO.

==================================================================
TRECHOS DA RAG:
{contexto_normativo}

EVENTO:
{descricao_evento}

N√ì ATUAL:
ID: {node_id}
Pergunta: {pergunta_atual}

FILHOS:
{filhos_formatados}
==================================================================

Avalie CADA FILHO:

- Compat√≠vel ‚Üí 0.6 a 1.0
- Incompat√≠vel ‚Üí 0.0 a 0.1
- Incerteza ‚Üí 0.2 a 0.4

Justificativa **sempre coerente com score**.

Retorne **apenas JSON**:

{{
  "avaliacoes": [
    {{"id": "...", "score": 0.0, "justificativa": "..."}}
  ]
}}
""")

    # Montar prompt completo
    full_prompt = prompt.format(
        contexto_normativo=contexto_normativo,
        descricao_evento=descricao_evento.strip(),
        node_id=node["id"],
        pergunta_atual=node.get("pergunta", ""),
        filhos_formatados=filhos_formatados,
    )

    # Usar invoke_json com retry autom√°tico
    try:
        print(f"üîÑ Chamando LLM para avaliar {len(filhos)} filhos...")
        data = invoke_json(
            llm_json,
            full_prompt,
            max_retries=2,
            schema_hint='{"avaliacoes": [{"id": "...", "score": 0.0, "justificativa": "..."}]}'
        )
        print(f"‚úÖ LLM respondeu com sucesso")
    except Exception as e:
        print(f"[ERRO] Falha ao chamar LLM em avaliar_filhos_llm: {type(e).__name__}: {e}")
        import traceback
        print(traceback.format_exc())
        return []

    aval = data.get("avaliacoes", [])
    if not isinstance(aval, list):
        return []

    out = []
    for a in aval:
        try:
            out.append({
                "id": str(a["id"]),
                "score": float(a.get("score", 0.0)),
                "justificativa": str(a.get("justificativa", "")),
            })
        except:
            continue

    return out
